{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a42ef0-89d0-42a7-99d4-3f3bf9e22854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# This will get wiped each time the container is shutdown and is just for demo purposes \n",
    "warehouse_location = \"/tmp/spark-warehouse\"\n",
    "# Clean out each run of this cell for demo\n",
    "%rm -fr {warehouse_location}\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2a26a-b2fa-43d9-952c-d52e05a389b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory SQL DataFrame from JSON\n",
    "df = spark.read.format(\"json\").load(\"/sampledata/health_tracker_data_2020_01.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2b217-11a3-4e97-aff3-a86ebddfef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f1405-bef6-4fbd-8a1c-4ee4e5f31572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No tables yet\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d6695-6659-48c5-8187-b462160ce8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import column, date_format, from_unixtime\n",
    "\n",
    "# Create cleaner silver table, converting timestamp to a time and adding a date column\n",
    "df_silver = df \\\n",
    "  .withColumnRenamed(\"device_id\", \"p_device_id\") \\\n",
    "  .withColumn(\"timestamp\", from_unixtime(column(\"timestamp\"))) \\\n",
    "  .withColumn(\"date\", date_format(column(\"timestamp\"), format=\"y-MM-DD\"))\n",
    "df_silver.write.saveAsTable(\"health_tracker_silver\", format=\"delta\", partitionBy=\"p_device_id\", path=warehouse_location + \"/DLRS/healthtracker/silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e360b7-eea1-4834-9fba-731850575289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table in memory\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced126f-b14a-46e1-8dea-a398b7dc57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE DETAIL health_tracker_silver\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc281a6-608f-4cd9-8c19-f72ef3900efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are partitioned by requested column\n",
    "%ls {warehouse_location}/DLRS/healthtracker/silver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acd882-c803-4c96-ab8b-ce2bec94e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows {df_silver.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3fb44-b819-45fb-a543-2e1057505094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gold table/data mart for basic summary data. Generally created when the performance of SQL\n",
    "# queries on the silver tables is not good enough\n",
    "\n",
    "# SQL\n",
    "# CREATE TABLE health_tracker_user_analytics\n",
    "# USING DELTA\n",
    "# LOCATION 'warehouse_location/DLRS/healthtracker/gold/health_tracker_user_analytics'\n",
    "# AS (\n",
    "#   SELECT p_device_id,\n",
    "#          AVG(heartrate) AS avg_heartrate,\n",
    "#          STD(heartrate) AS std_heartrate,\n",
    "#          MAX(heartrate) AS max_heartrate\n",
    "#   FROM health_tracker_silver GROUP BY p_device_id\n",
    "# )\n",
    "\n",
    "# Python DataFrame API\n",
    "from pyspark.sql.functions import avg, max, stddev\n",
    "\n",
    "df_gold = df_silver.groupBy(\"p_device_id\").agg(avg(\"heartrate\"), max(\"heartrate\"), stddev(\"heartrate\"))\n",
    "df_gold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53855656-7ad6-4d4e-b49e-bee1b9a4e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update silver tables with data from February\n",
    "df_silver = spark.read.format(\"json\").load(\"/sampledata/health_tracker_data_2020_02.json\") \\\n",
    "  .withColumnRenamed(\"device_id\", \"p_device_id\") \\\n",
    "  .withColumn(\"timestamp\", from_unixtime(column(\"timestamp\"))) \\\n",
    "  .withColumn(\"date\", date_format(column(\"timestamp\"), format=\"y-MM-DD\")).union(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504aa278-5f21-4ffe-89fa-28e90fb956f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver.show()\n",
    "print(f\"Number of rows {df_silver.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80758ac-21e5-4c46-87cc-528a194b4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver.write.saveAsTable(\"health_tracker_silver\", mode=\"overwrite\", format=\"delta\", partitionBy=\"p_device_id\", path=warehouse_location + \"/DLRS/healthtracker/silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61aa006-6d4b-4ee8-9cdc-86467faa73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY health_tracker_silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8513-c6fe-4124-a354-54c293ad6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta tables retain their history\n",
    "spark.sql(\"SELECT COUNT(*) FROM health_tracker_silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7b3b2-5f3f-49f9-91be-6916acfbedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM health_tracker_silver VERSION AS OF 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006c533-ebab-449d-9001-e74198da2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next set of cells fixes and inserts new data to cope with common problems:\n",
    "#  - corrupted source data (in our case negative heartrate)\n",
    "#  - missing data (in our case we will load in the march data)\n",
    "# Accomplished in one shot with an \"upsert\"\n",
    "\n",
    "# View broken records\n",
    "df_broken = df_silver[df_silver.heartrate < 0.]\n",
    "df_broken.orderBy(\"p_device_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7734156-445f-4397-a3ca-7b015e64beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DF just containing values with updates\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, lead, monotonically_increasing_id\n",
    "\n",
    "df_tmp = df_silver\\\n",
    "    .withColumn(\"prev_hr\", lag(df_silver[\"heartrate\"]).over(Window.partitionBy(\"p_device_id\").orderBy(\"timestamp\")))\\\n",
    "    .withColumn(\"next_hr\", lead(df_silver[\"heartrate\"]).over(Window.partitionBy(\"p_device_id\").orderBy(\"timestamp\")))[df_silver.heartrate < 0.]\n",
    "df_updates = df_tmp.withColumn(\"heartrate\", 0.5*(df_tmp[\"prev_hr\"] + df_tmp[\"next_hr\"])).select(\"p_device_id\", \"heartrate\",\"name\", \"timestamp\", \"date\")\n",
    "df_updates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1c265-34c0-4d3a-a4a9-9084553d6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DF with values to insert.\n",
    "# In this case for the demo we will ignore the broken data as we just want a dataset to insert\n",
    "df_inserts = spark.read.format(\"json\").load(\"/sampledata/health_tracker_data_2020_03.json\") \\\n",
    "  .withColumnRenamed(\"device_id\", \"p_device_id\") \\\n",
    "  .withColumn(\"timestamp\", from_unixtime(column(\"timestamp\"))) \\\n",
    "  .withColumn(\"date\", date_format(column(\"timestamp\"), format=\"y-MM-DD\"))\n",
    "df_inserts = df_inserts[df_inserts.heartrate > 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e153d-a279-402d-8efb-26049bad437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an upserts DF containing all data for corrections\n",
    "df_upserts = df_updates.union(df_inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca1e93-10ae-418c-92f2-a95b55711e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the existing silver table. This only works for delta tables.\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "print(f\"Number of silver records before upsert: {df_silver.count()}\")\n",
    "# Create DeltaTable object from existing silver delta table \n",
    "delta_table_silver = DeltaTable.forName(spark, \"health_tracker_silver\")\n",
    "\n",
    "delta_table_silver.alias(\"silver\") \\\n",
    "  .merge(df_upserts.alias(\"upserts\"), \n",
    "        \"silver.p_device_id = upserts.p_device_id and silver.timestamp = upserts.timestamp\") \\\n",
    "  .whenMatchedUpdate(set = {\n",
    "    \"heartrate\": \"upserts.heartrate\"\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values = {\n",
    "    \"p_device_id\": \"upserts.p_device_id\",\n",
    "    \"heartrate\": \"upserts.heartrate\",\n",
    "    \"name\": \"upserts.name\",\n",
    "    \"timestamp\": \"upserts.timestamp\",\n",
    "    \"date\": \"upserts.date\"\n",
    "  })\\\n",
    "  .execute()\n",
    "\n",
    "df_silver = delta_table_silver.toDF()\n",
    "print(f\"Number of silver records after upsert: {df_silver.count()}\")\n",
    "print(f\"Number of broken records after upsert: {df_silver[df_silver.heartrate < 0.].count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2c7e7-7baa-4dc4-bbb2-7f9ddb798b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver[df_silver.p_device_id == 0].toPandas().plot(x=\"timestamp\", y=\"heartrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b7776-e0c9-4d06-bc79-32db855d1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY health_tracker_silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318e484-0f86-493a-aeeb-e1d24fe592e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
