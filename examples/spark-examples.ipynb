{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ca013-4723-4754-9b83-525702d5df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook provides basic examples are interacting with Spark through the pyspark API\n",
    "# If it is run with the corresponding Docker compose file then the Web UI at http://localhost:4040\n",
    "# shows details of the jobs executed on the cluster/local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a86039-0c68-4298-8e35-980f8022823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import requests\n",
    "\n",
    "# Fetch sample data\n",
    "SAMPLE_DATA_URL = 'https://github.com/owid/owid-datasets/raw/master/datasets/UK%20Nominal%20wage%20data%2C%20price%20data%2C%20and%20real%20wage%20%E2%80%93%20Bank%20of%20England%20(!Three%20centuries%20of%20macroeconomic/UK%20Nominal%20wage%20data%2C%20price%20data%2C%20and%20real%20wage%20%E2%80%93%20Bank%20of%20England%20(!Three%20centuries%20of%20macroeconomic.csv'\n",
    "SAMPLE_DATA_CSV = abspath('uk-macroeconomic-data.csv')\n",
    "\n",
    "resp = requests.get(SAMPLE_DATA_URL)\n",
    "with open(SAMPLE_DATA_CSV, 'wb') as file_handle:\n",
    "    file_handle.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f5af4-4012-453c-a385-3c8c08eb7b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69962da1-8584-46b0-8236-9bc7e52bc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data into Spark DataFrame.\n",
    "# By default all columns are assumed to be strings and column names are taken from header if provided\n",
    "df_str = spark.read.csv(SAMPLE_DATA_CSV, header=True, sep=\",\")\n",
    "df_str.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f188f0-f7cd-4efb-a307-c9aad163da5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CSV data into Spark DataFrame and infer the schema. Note that this requires an\n",
    "# additional pass over the data so is not suited for large datasets\n",
    "df_inferred = spark.read.csv(SAMPLE_DATA_CSV, header=True, inferSchema=True, sep=\",\")\n",
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0742a-b339-4326-b903-86b665391531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CSV data into Spark DataFrame and provide the schema.\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Nominal Average Weekly Wages (2017)\", DoubleType(), True),\n",
    "    StructField(\"Spliced CPI (2015=100)\", DoubleType(), True),\n",
    "    StructField(\"Real Average Weekly Wages (2017)\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "df_manual_schema = spark.read.csv(SAMPLE_DATA_CSV, header=True, schema=macroeconomic_schema, sep=\",\")\n",
    "df_manual_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4db935-5acb-4ecc-be79-5c1a0fee5386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQL queries can be run on the data by create a temporary view for this.\n",
    "# The lifetime is tied to the session\n",
    "\n",
    "# DF API\n",
    "filtered_df = df_manual_schema.filter(df_manual_schema[\"Real Average Weekly Wages (2017)\"] > 200.)\n",
    "print(f'No. of Years where Real Average Weekly Wages > £200 (DataFrame API): {filtered_df.count()}')\n",
    "\n",
    "# SQL query\n",
    "df_manual_schema.createOrReplaceTempView('economy')\n",
    "sql_df = spark.sql(\"select * from economy where `Real Average Weekly Wages (2017)` > 200\")\n",
    "print(f'No. of Years where Real Average Weekly Wages > £200 (SQL API): {sql_df.count()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
